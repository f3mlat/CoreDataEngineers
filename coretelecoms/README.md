# CoreTelecoms Data Platform

Modern Data Engineering with **Airflow, Amazon(IAM, S3, SSM), Google(IAM, GCS, BigQuery), dbt, and Docker.**

---

## ğŸ“Œ Project Overview

The **CoreTelecoms Data Platform** is an end-to-end modern data engineering solution designed to ingest, transform, and deliver analytics-ready telecom datasets using a fully containerized, **metadata-driven** architecture.

The platform supports:

* Automated ingestion from different sources - Amazon S3, Google sheet, and PostgreSQL database to Google Cloud Storage
* Metadata-driven ingestion orchestration using Airflow
* Scalable transformations modeled with dbt (staging â†’ intermediate â†’ marts)
* Partitioned + clustered BigQuery optimized tables
* Local reproducibility using Docker Compose

---

## ğŸ— Architecture Diagram

### **High-Level Architecture (ASCII)**

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Upstream Telecom Sources â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                  Google Cloud Storage (Datalake)
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        Airflow DAGs      â”‚
                â”‚  - Ingestion Pipelines   â”‚
                â”‚  - dbt Run/Test/Docs     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ BigQuery Data Warehouse  â”‚
                â”‚  â€¢ Raw Layer             â”‚
                â”‚  â€¢ Staging Layer (dbt)   â”‚
                â”‚  â€¢ Intermediate Layer    â”‚
                â”‚  â€¢ Datawarehouse         |
                |    (Analytics)           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     BI / Analytics       â”‚
                â”‚  Looker / Tableau / DS   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ airflow
â”‚Â Â  â”œâ”€â”€ config
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ airflow.cfg
â”‚Â Â  â”‚Â Â  â””â”€â”€ metadata.yml                    # metadata of sources and their attributes
â”‚Â Â  â”œâ”€â”€ dags
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ bigquery_table_creation_dag.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dbt_transfrom_dag.py            # dbt run/test/docs orchestration
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ full_pipeline.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ ingestion_dag.py                # Metadata-driven ingestion DAG
â”‚Â Â  â”œâ”€â”€ dbt_transfrom_dag.py
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â””â”€â”€ requirements.txt
â”œâ”€â”€ dbt
â”‚Â Â  â”œâ”€â”€ dbt_project.yml
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ macros
â”‚Â Â  â”‚Â Â  â””â”€â”€ get_customer_age_group.sql
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ datawarehouse
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ customer_experience
â”‚Â Â  â”‚Â Â  â”‚Â Â      â”œâ”€â”€ dim_agents.sql
â”‚Â Â  â”‚Â Â  â”‚Â Â      â”œâ”€â”€ dim_customers.sql
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ fact_complaints.sql
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ intermediate
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ int_all_complaints.sql
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ int_customer_age_group.sql
â”‚Â Â  â”‚Â Â  â””â”€â”€ staging
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ schema.yml
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ stg_agents.sql
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ stg_call_center.sql
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ stg_customers.sql
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ stg_social_media.sql
â”‚Â Â  â”‚Â Â      â””â”€â”€ stg_web_complaints.sql
â”‚Â Â  â”œâ”€â”€ profiles.yml
â”‚Â Â  â””â”€â”€ tests
â”‚Â Â      â””â”€â”€ test_complaint_keys.sql
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .github
â”‚Â Â  â””â”€â”€ workflows
â”‚Â Â      â”œâ”€â”€ cd.yml
â”‚Â Â      â””â”€â”€ ci.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ ingestion
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ extractors
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ google_sheets_extract.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ metadata_append.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ postgres_extract.py
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ s3_extract.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ requirements.txt
â”‚Â Â  â”œâ”€â”€ load
â”‚Â Â  â”‚Â Â  â””â”€â”€ load_to_gcs.py
â”‚Â Â  â”œâ”€â”€ run_ingestion.py
â”‚Â Â  â””â”€â”€ utils
â”‚Â Â      â””â”€â”€ gcp_client.py
â””â”€â”€ terraform
    â”œâ”€â”€ gcs.tf
    â”œâ”€â”€ iam.tf
    â”œâ”€â”€ main.tf
    â”œâ”€â”€ policy.tf
    â”œâ”€â”€ s3.tf
    â””â”€â”€ variables.tf
```

---

## ğŸ›  Tools & Technologies

| Layer                           | Tool                                        |
| --------------------------------| --------------------------------------------|
| Orchestration                   | **Apache Airflow**                          |
| Data Warehouse                  | **Google BigQuery**                         |
| Storage                         | **Amazon S3**, **Google Cloud Storage**     |
| Transformations                 | **dbt-core + dbt-bigquery**                 |
| Runtime Environment             | **Docker + Docker Compose**                 |
| Metadata                        | YAML-based ingestion configs                |
| Infrastructure provisioning     | Terraform                                   |

---

## ğŸš€ Setup Instructions

### **1. Clone the Repository**

```bash
git clone https://github.com/f3mlat/CoreDataEngineers.git
cd coretelecoms
```

---

### **2. Create Service Account + Key**

```bash
gcloud iam service-accounts create airflow-sa \
  --display-name "Airflow Service Account"

gcloud projects add-iam-policy-binding <PROJECT_ID> \
  --member "serviceAccount:airflow-sa@<PROJECT_ID>.iam.gserviceaccount.com" \
  --role "roles/bigquery.admin"

gcloud iam service-accounts keys create credentials/<credentials_file>.json \
  --iam-account airflow-sa@<PROJECT_ID>.iam.gserviceaccount.com
```

Place the key in:

```
/.google/creds/<credentials_file>.json
```

---

### **3. Start the Platform**

```bash
docker-compose up -d --build
```

Airflow UI â†’ **[http://localhost:8080](http://localhost:8080)**

Credentials â†’ `admin / admin`

---

### **4. Validate dbt Connection**

```bash
docker exec -it coretelecome-airflow-apiserver-1 bash
dbt debug --project-dir /opt/airflow/dbt --profiles-dir /opt/airflow/dbt
```
---

## ğŸ§© Metadata-Driven Pipelines

Ingestion metadata lives in:

```
dags/metadata/pipelines.yml
```

Each dataset metadata defines:

* Type of source and storage - s3_csv, s3_json
* Bucket of source in cloud storage
* Key which details the folder_name/source_file_name
* Frequency of source - static, daily
* Destination path - GCS

This enables **zero-code ingestion changes** and **scalability**.

---

## ğŸ“Š dbt Modeling Layers

```
staging â†’ intermediate â†’ datawarehouse
```

### Includes:

* Source standardization
* Cleansing
* Feature enrichment
* Business-ready fact + dimension models
* Partitioned + clustered BigQuery tables

Example:

```sql
{{ config(
    materialized='table',
    partition_by={'field': 'ingestion_date', 'data_type': 'date'},
    cluster_by=['customer_state']
) }}
```

---

## ğŸ“¦ Airflow DAGs

### **1. ingestion_dag.py**

* [Amazon S3, Google Sheet, PostgreSQL] â†’ GCS raw ingestion
* Metadata-driven loader
* Automatic task generation

### **2. bigquery_table_creation_dag.py**

* Creates external table from parquet files
* Creates regular tables

---

### **3. dbt_transform_dag.py**

* Runs `dbt run`
* Runs `dbt test`
* Builds documentation

---

### **4. full_pipeline_dag.py**

* Runs the above 3 dags at a scheduled time

---

### Running the Pipeline

- **Trigger the DAG(s) Manually**
    Via Airflow UI:
    - Navigate to http://localhost:8080
    - Find the DAG - coretelecoms_full_pipeline
    - Toggle it ON
    - Click the "Trigger" button

---

## ğŸ§ª Testing

```bash
dbt test
```

---

## ğŸ“˜ Documentation

Generate and host dbt docs:

```bash
dbt docs generate
dbt docs serve
```

---

## ğŸ›  Troubleshooting

### âŒ BigQuery Authentication Error

Ensure correct profiles.yml:

```yaml
keyfile: /opt/airflow/.cred/<credentials_file>.json
```

---

## ğŸ‘¤ Author / Maintainer

Lateef â€“ Data Engineer

---

## ğŸ‰ Final Notes

This repository demonstrates a modern data engineering stack aligned with enterprise best practices.
It is fully reproducible, easily extendable, and ready for both learning and real-world deployment.

---