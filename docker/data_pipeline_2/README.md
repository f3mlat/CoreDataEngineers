## Dockerized ELT Pipeline
### ğŸ“ŒOverview

This project demonstrates a fully automated ELT pipeline using Docker.
The pipeline consists of three containers/services:

1. **Python (ETL)** â†’ Extracts CSV data from a URL and loads it into Postgres.
2. **Postgres** â†’ Staging database to hold raw and transformed data.
3. **DBT** â†’ Transforms the staged data into a clean analytics model.

The pipeline is scheduled to run **daily at 12:00 a.m.** using a cron job.


## ğŸ— Architecture
![Pipeline Architecture](diagrams/pipeline_architecture.png)


## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ dbt
â”‚Â Â  â”œâ”€â”€ dbt_project.yml
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ export_var.sh
â”‚Â Â  â”œâ”€â”€ logs
â”‚Â Â  â”‚Â Â  â””â”€â”€ dbt.log
â”‚Â Â  â”œâ”€â”€ models
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ analytics
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ dim_finance_summary.sql
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ schema.yml
â”‚Â Â  â”‚Â Â  â””â”€â”€ staging
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ schema.yml
â”‚Â Â  â”‚Â Â      â””â”€â”€ stg_finance.sql
â”‚Â Â  â””â”€â”€ profiles.yml
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ etl
â”‚Â Â  â”œâ”€â”€ Dockerfile
â”‚Â Â  â”œâ”€â”€ etl.py
â”‚Â Â  â”œâ”€â”€ extract_data.py
â”‚Â Â  â”œâ”€â”€ load_data.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ run_etl.sh
â”œâ”€â”€ raw.csv
â”œâ”€â”€ README.md
â””â”€â”€ scripts
    â”œâ”€â”€ export_var.sh
    â””â”€â”€ run_pipeline.sh
```

## âš™ï¸ Prerequisites
- Linux environment
- Bash shell
- Docker Docker & Docker Compose
- Python
- git
- PostgreSQL client (optional, for testing)


## ğŸš€ Running the Pipeline
### 1. Build & Start Containers
```shell
source scripts/run_pipeline.sh
```
This will:
- Starts Postgres container
- Starts and run the Python ETL container (extract + load)
- Starts and run DBT container (transform data)

### 2. Verify Data

You can connect to Postgres locally:

psql -h localhost -U user -d <database_name>

### 3. Schedule Daily Run

Edit crontab with crontab -e and add:

0 0 \* \* \* /home/l/etl-pipeline-docker/scripts/run_pipeline.sh >> /home/l/etl-pipeline-docker/logs/pipeline.log 2>&1

This ensures the pipeline runs every midnight.

### Dockerized ETL Lifecycle
1. **Extract (Python)**Â â†’ download and clean data
2. **Load (Postgres)**Â â†’ save to staging database
3. **Transform (dbt)**Â â†’ create analytics-ready models
4. **Schedule (cron)**Â â†’ run daily automatically


